MODEL_NAME = "meta-llama/Meta-Llama-3.1-8B-Instruct" # Pre-trained model
DATASET_NAME = "HuggingFaceH4/instruction-dataset" # Path of dataset for training and evaluation
TRAIN_BATCH_SIZE = 64 # Number of samples processed before the model's internal parameters are updated during training
EVAL_BATCH_SIZE = 64 # Number of samples processed during evaluation to calculate metrics
GRADIENT_ACCUMULATION_STEPS = 10 # Number of steps for which gradients are accumulated before performing a backward/update pass
LEARNING_RATE = 0.005 # Step size at each iteration while moving toward a minimum of the loss function
NUM_TRAIN_EPOCHS = 70 # Number of times the entire dataset is passed through the model during training
WARMUP_STEPS = 50 # Number of steps for gradually increasing the learning rate from 0 to the set value
MAX_SEQ_LENGTH = 512 # The maximum sequence length for input data; longer sequences will be truncated
TRAIN_RATIO = 0.8 # The ratio of the dataset to be used for training; the remainder is used for validation/testing
